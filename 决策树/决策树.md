<!-- TOC -->

- [基本流程](#基本流程)
- [划分选择](#划分选择)
  - [ID3算法](#ID3算法)
  - [RDF序列化方法](#RDF序列化方法)
- [RDF的“衣服”——RDFS/OWL](#RDF的“衣服”——RDFS/OWL)
- [RDFS](#RDFS)
- [OWL](#OWL)
  - [数据建模能力](#数据建模能力)
  - [推理能力](#推理能力)
<!-- /TOC-->
--------------------------------------------------

## 基本流程
<div align="center"><img src="./picture/决策树图一.png" height="" /></div>

- 决策树基于**树结构**进行决策，决策过程的每个判定问题都是对某个属性的“测试”；
- 学习目的：为了的产生一棵**泛化能力强**，即处理未见示例能力强的决策树；
- 基本流程遵循"**分而治之**"的思想。

<div align="center"><img src="./picture/流程图.png" height="" /></div>

- 决策树的生成是一个**递归**的过程
- 有三种情况会导致递归返回
    - 当前节点所包含的样本**全部属于同一类**，无需划分 。这时将结点化为叶子结点，样本属于该类别；
    - 属性集为空或者数据集在当前属性集上所有取值相同，无法划分 。这时将结点化为叶子结点并将样本**归属于多数类**；
    - 当前节点所包含的样本集合为空，不能划分。这时将结点化为叶子结点并将样本归属于**父节点**的多数类。

--------------------------------------------------

## 划分选择
- 决策树学习的关键是**如何选择最优划分属性**
- 我们希望决策树的分支结点所包含的样本尽量属于同一类别；
- 即结点的"纯度（purity）"越来越高。

### ID3算法
#### 信息熵
- 是度量样本集合纯度最常用的一种指标。

<div align="center"><img src="./picture/信息熵.png" height="" /></div>

- Ent(D)的值越小，包含的信息量就越小，D的纯度就越高。

#### 信息增益
- 假定离散属性a有V个取值{a1...aV}；
- 若使用a对样本集D进行划分，会产生V个分支，其中第v个包含了D中在a属性上取值为av的所有样本，记为Dv；
- 再根据不同分支结点包含的样本数不同来给与权重：|Dv|/|D|
- 如此一来，属性a对样本进行划分所得的信息增益就表示为：

<div align="center"><img src="./picture/信息增益.png" height="" /></div>

