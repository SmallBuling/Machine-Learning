<!-- TOC -->

- [基本流程](#基本流程)
- [划分选择](#划分选择)
  - [ID3算法](#ID3算法)

<!-- /TOC-->
--------------------------------------------------

## 基本流程
<div align="center"><img src="./picture/决策树图一.png" height="" /></div>

- 决策树基于**树结构**进行决策，决策过程的每个判定问题都是对某个属性的“测试”；
- 学习目的：为了的产生一棵**泛化能力强**，即处理未见示例能力强的决策树；
- 基本流程遵循"**分而治之**"的思想。

<div align="center"><img src="./picture/流程图.png" height="" /></div>

- 决策树的生成是一个**递归**的过程
- 有三种情况会导致递归返回
    - 当前节点所包含的样本**全部属于同一类**，无需划分 。这时将结点化为叶子结点，样本属于该类别；
    - 属性集为空或者数据集在当前属性集上所有取值相同，无法划分 。这时将结点化为叶子结点并将样本**归属于多数类**；
    - 当前节点所包含的样本集合为空，不能划分。这时将结点化为叶子结点并将样本归属于**父节点**的多数类。

--------------------------------------------------

## 划分选择
- 决策树学习的关键是**如何选择最优划分属性**
- 我们希望决策树的分支结点所包含的样本尽量属于同一类别；
- 即结点的"纯度（purity）"越来越高。

### ID3算法
#### 信息熵
- 是度量样本集合纯度最常用的一种指标。

<div align="center"><img src="./picture/信息熵.png" height="" /></div>

- **Ent(D)的值越小，包含的信息量就越小，D的纯度就越高。**

#### 信息增益
- 假定离散属性a有V个取值{a1...aV}；
- 若使用a对样本集D进行划分，会产生V个分支，其中第v个包含了D中在a属性上取值为av的所有样本，记为Dv；
- 再根据不同分支结点包含的样本数不同来给与权重：|Dv|/|D|
- 如此一来，属性a对样本进行划分所得的信息增益就表示为：

<div align="center"><img src="./picture/信息增益.png" height="" /></div>

- 一般而言，信息增益越大，那么用属性a进行划分所得到的纯度提升就越大。

<div align="center"><img src="./picture/西瓜数据集.png" height="" /></div>

- 西瓜实例有17个，以此为例对上述内容进行计算：

<div align="center"><img src="./picture/Ent(D).png" height="" /></div>

- 分别计算属性集合{色泽，根蒂，敲声，纹理，脐部，触感}每个熟悉的信息增益；
- 以属性"色泽"为例，3个可能取值{青绿，乌黑，浅白}；
- D1(色泽=青绿)，D2(色泽=乌黑)，D3(色泽=浅白)。
- 分别计算三个属性的信息熵

<div align="center"><img src="./picture/色泽信息熵.png" height="" /></div>

- "色泽"的信息增益

<div align="center"><img src="./picture/色泽信息增益.png" height="" /></div>

- 类似的，我们可以计算其他属性的信息增益

<div align="center"><img src="./picture/其他属性信息增益.png" height="" /></div>

- 属性"纹理"的信息增益最大，于是它被划选为划分属性

<div align="center"><img src="./picture/纹理.png" height="" /></div>

- 然后对每个分支结点做进一步划分；
- 以图4.3的第一个分支为例(纹理=清晰)
    - 包含的样例集合D1中有编号{1，2，3，4，5，6，8，10，15};
    - 可用属性集合{色泽，根蒂，敲声，脐部，触感}
    - 计算各属性的信息增益：

<div align="center"><img src="./picture/其他属性信息增益2.png" height="" /></div>

- "根蒂","脐部","触感"均取得了最大的信息增益，任选其一作为划分属性
- 同理最后可得决策树：

<div align="center"><img src="./picture/数据集2.0生成的决策树.png" height="" /></div>

### C4.5算法
##### 增益率
- 以编号为例，17个样本中有17个编号，显然可以以此为分支，且这些分支的纯度最大。然而，这样的决策树现在不具有泛化能力；
- 信息增益对**可取较多数目的属性**有明显的偏好，为了减少这样的偏好带来的不利影响，C4.5使用增益率来选择最优划分属性；
- C4.5算法不直接采用信息增益，而是使用"增益率"来选择最优划分属性。
- 增益率的定义：

<div align="center"><img src="./picture/增益率.png" height="" /></div>

- IV(intrinsic value)称为固有值，属性a可取值越多，固有值就越大；
- 以属性"触感"为例：
    - (触感=硬滑)：12/17
    - (触感=软粘)：5/17
    - 由(公式4.4)求得：IV(触感) = 0.874
    - 同理可得：IV(色泽) = 1.580，IV(编号)=4.088
- **增益率准则对属性较少的属性有所偏好，所以C4.5不直接使用增益率最大的候选划分属性，而是先选出信息增益高于平均值的属性，再从中选择增益率最高的一个**。

### CART算法
#### 基尼指数（Gini index）
- CART决策树中，使用基尼指数来选择划分属性。
- 数据集D的纯度可以用基尼值来度量。
- 基尼值定义：

<div align="center"><img src="./picture/基尼值.png" height="" /></div>

- 基尼指数定义：

<div align="center"><img src="./picture/基尼指数.png" height="" /></div>

- 开始看书看

